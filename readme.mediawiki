
==概要==
[http://www.ne.jp/asahi/hishidama/home/tech/apache/hadoop/index.html Hadoop]の各種方式でCSVファイルの集計を行うサンプルを作ってみた。

===実行環境===
実行環境は[https://github.com/hishidama/Hadoop-example1-WordCount WordCountのサンプル]を試した環境と同じ。<br>
（つまり仮想マシンを使った分散環境なので、実行速度は参考程度）

===サンプルの仕様===
CSVファイルの色々な項目をキーとして、複数種類の集計を行ってみる。<br>
意図としては、同一の入力ファイルを使って別々の異なる集計を行う、というもの。

もう少し具体的には、販売履歴をイメージしたCSVファイルから以下のような集計を行う。
{|border="1"
|+ align="center"|CSVファイルのレイアウト
|日付 ||時刻 ||伝票番号 ||商品コード ||商品の値段 ||商品の個数
|}
*日付項目は持っているが、ファイルには1日分しか入ってこないものとする。
*同一の伝票番号に複数商品が入る。（複数レコードにまたがる）

;その日の総売上
:日付毎に売上（値段×個数）を合算（出力は1レコード）
;その日の客数
:日付毎に伝票番号をdistinctして、その個数（出力は1レコード）
;時間帯毎の売上
:時刻「hh:mm:ss」を「hh」に変換し、日付・hh毎に売上（値段×個数）を合算<br>ただし、売上が無い時間帯には0のレコードを出力する（つまり必ず24レコードになる）
;その日に売れた商品ベスト10
:日付・商品コード毎に売上（値段×個数）を合算し、売上の降順で10レコード出力する

===実装方針===
基本的に各言語の主要機能だけを利用してコーディングする。（例えばPigやHiveのUDFは極力作らない）<br>
つまり、同じ仕様を実現するにも、プロダクト間でその実装方式は異なる。


==実行速度==
{|border="1"
!方式（プロダクト）
!仮想分散環境
|-
|Pig
|3:12～3:24
|-
|Hive
|3:34～3:43
|-
|Java（MapReduce API）
|2:38～2:45
|-
|Java（改善版）
|2:30～2:39
|-
|Asakusa Framework
|1:49～1:54
|-
|Cascading
|3:31～3:45
|-
|AZAREA-Cluster Framework
|OutOfMemoryError
|}

CSVファイルは160MB（3ブロック分）。レコード数は約418万3千件。

実行時間には、ローカルファイルをHDFS上に転送して処理し、結果をローカルへ転送する時間まで全てを含む。<br>
（AsakusaFWのWindGateがそういう仕組みなので、他のプロダクトでもそれに合わせた）<br>
ちなみに160MBを転送するのは8秒くらいのようだった。


==所感==

===コーディングのポイント===
*総売上は単純な合算処理。
*客数はdistinctをとる。
**PigやHiveにはdistinct命令がある。CascadingはGroupBy＋Countで代用。
*時間帯別売上は売上0のレコードを追加する必要がある。
**PigやHiveでは時間帯の値（00～23）を持つテーブルを作成し、それと結合する方法を採った。
*TOP10
**ソートする。
**出力レコード数を制限する。Pig・Hive・Cascadingにはlimit命令がある。
**制限する件数をパラメーターとして渡す。

===Pig（0.8.1）===
今回の仕様（キー毎に集計する）程度だと、[http://www.ne.jp/asahi/hishidama/home/tech/apache/pig/index.html Pig]で書くのが一番分かりやすかった。<br>
（一番最初にPigで書き、他のプロダクトではそれを見ながら書いていった(笑)）

実行速度も微妙にHiveより速かった。<br>
CSVファイルは3ブロック分なのに、MapTaskが3つだったり4つだったりするのが不思議だが…。

===Hive（0.7.1）===
やはり[http://www.ne.jp/asahi/hishidama/home/tech/apache/hive/index.html Hive]はcreate tableと処理本体のスクリプトを分けたり、その中のファイルのパスを単体試験環境と分散環境用で変更するのがちょっと面倒。<br>
今回はローカルファイルを扱うということで、データファイルの置き場を/tmp/hadoop2に固定していたから書き換える部分は少なかったけど。

また、プログラムで使うパラメーターを外部から渡せないのはHiveだけ。（TOP10の「10」）

それと、Hiveはファイルへの出力がCSVに対応していないので、select時にconcatを使ってカンマ区切りの文字列に変換した。<br>
（concat_wsを使えばもう少し楽そうだったが、concat_wsは引数がStringにしか対応していないようだった。concatは数値でも使える）

それにしても、実行時にMapTaskが2つしかないのが不思議。テーブルへのロード時に何か特殊なことをしているのだろうか。

===Java（JDK1.6）===
MapReduceを直接書くのはやっぱりちょっと大変(苦笑)<br>
（というか、やっぱりWritableとかComparator書くの面倒＾＾；）

実行速度的にはPig・Hive・Cascadingより速いものが出来た。<br>
（MapTaskは素直に3つ動いている）

上位売上の処理は、当初作ったものは素直に2ジョブに分けていたが、改善して1ジョブに減らすことが出来て、少し速くなった。<br>
（基本的にはジョブが少ない方が全体の速度は速くなる）

===Asakusa Framework（0.2.4 WindGate）===
[http://www.ne.jp/asahi/hishidama/home/tech/asakusafw/windgate.html WindGate]がCSVファイルに対応したので、それを使ってみた。

さらに今回は、DMDLファイルを作るExcelマクロを組むという余計な事をしてみた(爆)<br>
つまり、Excelファイル上でファイルレイアウトを管理し、ついでにそこからDMDLを生成しようとする試み。<br>
しかしやってみたら、あまり筋は良くない感じ(苦笑)<br>
一番最初の入力ファイルと最後の出力ファイルのレイアウトだけだったらExcelで管理するのもいいかもしれないが、
DMDLでは途中で使う中間モデルのレイアウトも記述する必要があり、それらはExcel上で書くよりはテキストファイルに直接書く方が楽な気がする。<br>
Excelにしていて楽だったのは、参照元モデルの名前を変更したいときくらいかな（セルの参照機能を使ったので）。

実行速度はAsakusaFWが最速だった。<br>
これは、同一データを入力として複数種類の集計を行う場合、1つのMap/Reduce処理の中で複数種類の集計をまとめて行うことにより、MapReduceの段数を減らすという最適化が行われている為と思われる。<br>
（4種類集計しているのに、2ジョブしか無い。最初のジョブだけで1分くらいかかっているが、Java版が1ジョブ20～40秒かかっていることを考えれば、4ジョブ実行するよりは速い。<br>
そして、同様の集計の種類を増やしても、たぶんジョブ数は増えない）

また、MapTaskは4つになっていた。<br>
WindGateではローカルのCSVファイルを読み込んでHDFSにはSequenceFileとして書き込むので、サイズが大きくなって分散数が増えたのだと思われる。

あと、WindGateによるローカルへのファイル転送は、ディレクトリーでなくファイル名が直接指定できる。<br>
（AsakusaFW以外では、結果の出力先の指定はディレクトリーで、ファイル名はpart-r-00000とかの数字付きとなる）

===Cascading（1.2）===
[http://www.ne.jp/asahi/hishidama/home/tech/apache/hadoop/cascading/index.html Cascading]は、実行速度的にはHiveとほぼ同等か。

今回、入力データの価格項目と個数項目から金額項目を新たに生成する必要があったのだが、Cascadingでそれを上手くやる方法が分からなかった(苦笑)<br>
自前のFunctionを作らなくてもいい方法が用意されていそうな気がするのだが…。

また、出力レコードを制限するLimitは、複数タスクに分かれると上手く機能しないはずだが、今回はたまたま1タスクになったからそのまま使えた。

それと、ソート項目も複数指定することが出来るが、そのうち一部だけを降順にする方法が分からなかった。
（現在は日付・金額・商品コードの降順になっている。本当は金額だけ降順で、他は昇順にしたい）<br>
今回のテストデータではたまたま大丈夫そうに見えるが(苦笑)

これらの話は（Cascadingの日本語の資料が少ないから分からないので）、実際に業務でCascadingを使おうとしたら不安になるところ。

===AZAREA-Cluster Framework（0.9.0 評価版）===
[http://www.ne.jp/asahi/hishidama/home/tech/azarea/index.html AZAREA]は落ちた。

Hadoopのスタンドアローンモードで動かした場合は「Unexpected key」とかいう謎の例外。
（シミュレーションモードだとちゃんと正しい結果が出る）
Hadoopの分散環境で動かした場合はOutOfMemoryError。
ジョブ数は3つで、1つ目にかなり演算が集中している感じだが、それがいけないのだろうか。

==考察==
Cascadingは4種類の集計ジョブを同時に実行しているようだった。<br>
つまり、1台のデーターノードで複数タスク実行できるなら並列性が上がるし、1つのジョブがReduceフェーズに進めば他のノードが空くので、別ジョブのMapTaskが処理できる。
したがって、全体の速度が向上するのではないかと思う。<br>
PigやHiveはスクリプト言語である性質上、ジョブの並列化は出来なさそうな気がする。
（Pigのstore、Hiveのinsert/selectを受理した時点でMapReduceが始まるから。とは言え、スクリプトを全部解釈してから実行開始すれば、出来なくはないか）<br>
AsakusaFWがデフォルトで出力する実行シェルはシーケンシャルにジョブを実行する形だが、将来的には並列実行するようなものが出来るかも？<br>
（日立のJP1との連携では、そういうものが生成されるんじゃないかと思う）

今回、AsakusaFWを除けば、JavaでMap/Reduceを直接書いたものが一番速かった。
（Mapper/Reducer内でけっこう頑張るようにしたつもりだし）<br>
しかしAsakusaFWは複数ジョブ（MapReduce）にまたがる最適化をすることにより、一番速度が出た。<br>
もちろんそのロジックでMap/Reduceを直接書けば同等の速度は出ると思うが、生産性・難易度は全然違うだろうなー。<br>
AsakusaFWはそういった作り方をあまり考えなくても最適化してくれるので、すごい！（ステマｗ）

しかし[https://github.com/hishidama/Hadoop-example1-WordCount WordCountの比較]ではAsakusaFWは遅い部類だった＾＾；<br>
また、今回のケースでもPigやHiveで売上0の時間帯のレコードを追加する為にテーブル結合を利用したが、UDF（UDAF）を作れば速くなる可能性がある（かなり面倒そうだけど）。<br>
つまり重要なのは、'''「どの言語が実行性能が良い」とは一概には言えない'''ということ。<br>
対象業務の性質やアプリケーションの作り方によって速度に差が出る。まったく当たり前の話だけど＾＾；

==備考（実行方法）==
当サンプルを分散環境で実行させる方法。<br>
（[http://www.ne.jp/asahi/hishidama/home/tech/asakusafw/windgate.html#h_deploy AsakusaFWの実行環境]や[http://www.ne.jp/asahi/hishidama/home/tech/apache/hadoop/cascading/install.html#h_distribute Cascadingの実行環境]はインストールされている前提）

#[https://github.com/hishidama/Hadoop-example2-CSV/tree/master/sh shディレクトリー]をそのまま分散環境にコピーする。
#0data.shを実行して/tmp/hadoop2の下にテストデータを作成する。（それぞれの実行時にここからHDFSへ転送される。また、処理結果もここに格納される）
#2hive-init.shを実行し、Hiveのテーブルを作っておく。
#4afw-init.shを実行し、AsakusaFWアプリのアーカイブを解凍する。<br>また、WindGateのプロパティーファイル（$ASAKUSA_HOME/windgate/profile/asakusa.properties）内のresource.local.basePathも手動で/tmp/hadoop2に変えておく。
#それぞれのシェルを実行する。

